# ğŸï¸ Car Racing DQN Agent â€” AI Learns to Drive

This project trains a **Deep Q-Network (DQN)** agent to solve the **CarRacing-v3** environment from [Gymnasium](https://gymnasium.farama.org/).  
The agent learns to drive around procedurally generated tracks by **perceiving raw pixels** and optimizing long-term rewards.

<p align="center">
  <img src="https://gymnasium.farama.org/_images/car_racing.gif" width="420"/>
</p>

---

## ğŸ“– Background

CarRacing is a classic benchmark for reinforcement learning:

- **Observation**: top-down 96Ã—96 RGB image of the car and track.  
- **Action space**: continuous `[steering, gas, brake]` in `[-1,1] Ã— [0,1] Ã— [0,1]`.  
- **Challenge**: sparse rewards, exploration difficulty, long horizon, partial observability.  

We adapt this into a **discrete control problem** with wrappers, and train a **Dueling Noisy CNN-based DQN** agent.  
The key ideas:

1. **Convolutional encoder** â†’ extracts spatial features from frames.  
2. **Frame stacking** â†’ gives the agent a sense of motion.  
3. **Replay buffer** â†’ stores experience for stable learning.  
4. **Target network** â†’ reduces Q-value estimation oscillations.  
5. **Exploration** â†’ uses both epsilon-greedy and noisy linear layers.  
6. **Reward shaping** â†’ encourages speed, progress, and smooth driving while penalizing off-track behavior.  

---

## ğŸ“‚ Project Structure
```
.
â”œâ”€â”€ train.py           # Training loop with logging & checkpoints
â”œâ”€â”€ evaluate.py        # Run trained agent with rendering
â”œâ”€â”€ wrappers.py        # Preprocessing, frame stack, reward shaping, action discretization
â”œâ”€â”€ dqn_agent.py       # DQN agent class (policy, target net, optimizer)
â”œâ”€â”€ models.py          # CNN + Dueling + NoisyLinear architecture
â”œâ”€â”€ buffer.py          # Experience replay buffer
â”œâ”€â”€ utils.py           # Helpers (epsilon schedule, seeding, meters)
â”œâ”€â”€ requirements.txt   # Dependencies
â””â”€â”€ runs/              # Logs, checkpoints, and videos (auto-generated)
```

---

## âš™ï¸ Installation

```bash
# clone project
git clone https://github.com/akhilraj96/Car-Racing-DQN-Agent.git
cd carracing-dqn

# create virtual environment
conda create -p env python=3.10 -y
conda activate env/

# install dependencies
python -m pip install --upgrade pip setuptools wheel

pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128

pip install -r requirements.txt
pip install gymnasium[box2d] swig
```

Dependencies include:
- `gymnasium[classic-control, box2d]`
- `numpy`, `opencv-python`
- `torch`, `tensorboard`
- `imageio[ffmpeg, pyav]`
- `tqdm`

---

## ğŸš€ Training

Start training an agent:

- No video (fastest):
```bash
python train.py --video-mode none --run-name dqn_run1
```

- Watch live (slower, renders every step):
```bash
python train.py --video-mode live --run-name dqn_run1
```

- Record one episode every 50 episodes:
```bash
python train.py --video-mode record --record-every 50 --run-name dqn_run1
```

### Key arguments
- `--run-name NAME` â†’ name for logs/checkpoints (default: `dqn_run`)  
- `--total-steps N` â†’ number of environment steps (default: 1M)  
- `--video-mode [none|live|record]` â†’ rendering mode  
- `--save-every N` â†’ checkpoint frequency (default: 50k)  
- `--record-every N` â†’ save evaluation videos every N episodes  

Checkpoints and logs will be saved in:
```
runs/dqn_run1/checkpoints/
runs/dqn_run1/videos/
```

---

## ğŸ® Evaluation

Run a trained model:

```bash
python evaluate.py --checkpoint runs/dqn_run1/checkpoints/agent0/latest.pt --episodes 10
```

Options:
- `--episodes N` â†’ how many test runs  
- `--sleep T` â†’ delay between rendered frames  

---

## ğŸ“Š Monitoring

Training logs are tracked with **TensorBoard**:

```bash
tensorboard --logdir runs/
```

Youâ€™ll see:
- Episode returns  
- Per-step rewards  
- TD-loss curves  
- Exploration epsilon decay  

---

## ğŸ§© Environment Wrappers

To make learning tractable, we apply wrappers (`wrappers.py`):

1. **PreprocessFrame** â†’ grayscale + resize (84Ã—84)  
2. **FrameStack** â†’ stack last 4 frames  
3. **DiscreteActionWrapper** â†’ map continuous controls â†’ 9 discrete actions:
   - no-op, gas, brake  
   - left, right  
   - left+gas, right+gas  
   - left+brake, right+brake  
4. **StickyActions** â†’ repeat last action with prob 0.25 (adds stochasticity)  
5. **Reward shaping**:  
   - Base reward from CarRacing  
   - Speed bonus (encourages fast driving)  
   - Off-track penalty (discourages leaving road)  
   - Control penalties (penalize idling, braking, excessive steering)  
   - Progress reward (tile completion)  
   - Smoothness bonus (discourages jerky driving)  
   - Small time penalty  

---

## ğŸ§  Model Architecture

`DuelingNoisyCNN` (in `models.py`):

- **Convolutional encoder** (NatureCNN style):
  - Conv1: 32 Ã— 8Ã—8 stride 4  
  - Conv2: 64 Ã— 4Ã—4 stride 2  
  - Conv3: 64 Ã— 3Ã—3 stride 1  
- **Dueling heads**:
  - Value stream â†’ scalar V(s)  
  - Advantage stream â†’ A(s,a)  
  - Q(s,a) = V(s) + (A(s,a) âˆ’ mean(A))  
- **NoisyLinear layers** â†’ parametric exploration (Fortunato et al., 2018)  

This combination improves stability and exploration.

---

## ğŸ”¬ Algorithm

The training loop (`train.py`) implements:
- **Epsilon decay** â†’ from 1.0 â†’ 0.05 over 200k steps  
- **Replay buffer** â†’ 200k transitions  
- **Mini-batch training** â†’ batch size 64  
- **Discount factor** â†’ Î³ = 0.995  
- **Optimizer** â†’ Adam, LR = 1e-4  
- **Target network update** â†’ every 2000 steps  
- **TD loss** â†’ smooth L1 (Huber)  

---

## ğŸ“Š Results

> Your results will vary depending on seeds and hyperparams.

- With reward shaping, the agent learns to **stay on track** within ~200k steps.  
- By ~500k steps, it learns to **accelerate, steer smoothly, and complete laps**.  
- Average returns typically reach **600â€“800+** (solving threshold is 900).  

---

## ğŸ”® Future Work
- âœ… Double DQN  
- âœ… Prioritized Experience Replay (PER)  
- âœ… Dueling DQN (already included)  
- âœ… Noisy Networks for exploration (already included)  
- â¬œ Rainbow DQN integration  
- â¬œ Compare against PPO/SAC  

---

## ğŸ“œ License
MIT License. Free to use, modify, and share.




# Car-Racing DQN Agent ğŸï¸ğŸ’¨

This repository implements a **Deep Q-Network (DQN) agent** for the **CarRacing-v3** environment in Gymnasium.
The agent leverages **dueling noisy networks**, **frame stacking**, and **reward shaping** to learn smooth and efficient driving policies.

---

## ğŸ“¦ Project Structure

```
.
â”œâ”€â”€ buffer.py            # Replay buffer for experience replay
â”œâ”€â”€ dqn_agent.py         # DQN agent implementation with online and target networks
â”œâ”€â”€ evaluate.py          # Evaluate a trained agent with optional video recording
â”œâ”€â”€ models.py            # Neural network architectures (Dueling Noisy CNN)
â”œâ”€â”€ train.py             # Training script for the agent
â”œâ”€â”€ utils.py             # Utilities: epsilon schedule, seeding, average meter
â”œâ”€â”€ wrappers.py          # Environment wrappers: preprocessing, frame stacking, sticky actions, discrete actions
â”œâ”€â”€ requirements.txt     # Python dependencies
â””â”€â”€ README.md            # This file
```

---

## âš™ï¸ Installation

```bash
# Clone the repository
git clone https://github.com/akhilraj96/Car-Racing-DQN-Agent.git
cd Car-Racing-DQN-Agent

# Create and activate a virtual environment (Python 3.10)
conda create -p env python=3.10 -y
conda activate env/

# Upgrade pip and install dependencies
python -m pip install --upgrade pip setuptools wheel
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128
pip install -r requirements.txt
pip install gymnasium[box2d] swig
```

**Dependencies include:**

* `gymnasium[classic-control, box2d]`
* `numpy`, `opencv-python`
* `torch`, `tensorboard`
* `imageio[ffmpeg, pyav]`
* `tqdm`

---

## ğŸš€ Training

Start training the DQN agent in CarRacing-v3.

### Modes

* **No video (fastest training)**:

```bash
python train.py --video-mode none --run-name dqn_run1
```

* **Live visualization** (slower, renders every step):

```bash
python train.py --video-mode live --run-name dqn_run1
```

* **Record videos every N episodes**:

```bash
python train.py --video-mode record --record-every 50 --run-name dqn_run1
```

### Key Arguments

| Argument           | Description                                     |
| ------------------ | ----------------------------------------------- |
| `--run-name NAME`  | Name for logs/checkpoints (default: `dqn_run`)  |
| `--total-steps N`  | Total environment steps (default: 1,000,000)    |
| `--video-mode`     | Rendering mode: `none`, `live`, `record`        |
| `--save-every N`   | Checkpoint frequency in steps (default: 50,000) |
| `--record-every N` | Save video every N episodes (default: 10)       |

**Output directories:**

```
runs/dqn_run1/checkpoints/
runs/dqn_run1/videos/
```

---

## ğŸ® Evaluation

Evaluate a trained agent on the environment:

```bash
python evaluate.py --checkpoint runs/dqn_run1/checkpoints/latest.pt --episodes 10
```

### Evaluation Arguments

| Argument            | Description                                  |
| ------------------- | -------------------------------------------- |
| `--checkpoint PATH` | Path to saved model weights                  |
| `--episodes N`      | Number of episodes to evaluate (default: 10) |
| `--device`          | Run on `cpu` or `cuda`                       |
| `--video-mode`      | `none`, `live`, or `record`                  |

Recorded videos will be saved in `runs/videos/` if `--video-mode record` is selected.

---

## ğŸ§  Key Features

* **Dueling DQN with NoisyNet** for better exploration
* **Frame stacking** for temporal context
* **Reward shaping**:

  * Off-track penalty
  * Speed reward
  * Smooth driving penalty
  * Progress reward per track tile
* **Sticky actions wrapper** to handle environment stochasticity
* **TensorBoard logging** for training metrics:

  * Episode rewards
  * Step-wise rewards
  * TD loss
  * Epsilon schedule

---

## ğŸ–¥ï¸ Environment Wrappers

* `PreprocessFrame` â†’ Converts RGB frames to grayscale and resizes to 84x84
* `FrameStack` â†’ Stacks last 4 frames to capture temporal information
* `StickyActions` â†’ Repeats previous actions with a small probability
* `DiscreteActionWrapper` â†’ Maps continuous controls to 9 discrete actions

---

## ğŸ§© Neural Network

**Dueling Noisy CNN** architecture:

```
Input: (4 stacked frames, 84x84)
Conv layers: [32x8x8 stride 4] â†’ [64x4x4 stride 2] â†’ [64x3x3 stride 1]
Dueling streams:
  - Value: FC â†’ 512 â†’ 1
  - Advantage: FC â†’ 512 â†’ n_actions
NoisyLinear layers for exploration
```

---

## ğŸ“ˆ Training & Evaluation Tips

* Use `--video-mode none` for faster training without rendering
* Adjust epsilon decay (`--eps-start`, `--eps-end`, `--eps-decay-steps`) for exploration
* Check TensorBoard logs with:

```bash
tensorboard --logdir runs/dqn_run1
```

* Periodically evaluate the agent using `evaluate.py` to monitor progress

---

## ğŸ“œ References

* [DQN: Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236)
* [Dueling Network Architectures for Deep Reinforcement Learning](https://arxiv.org/abs/1511.06581)
* [Noisy Networks for Exploration](https://arxiv.org/abs/1706.10295)

---

## ğŸ“ License

This repository is open-source under the MIT License.
