# ğŸï¸ AI Learns to Drive â€” CarRacing-v3 DQN Agent

This project trains a **Deep Q-Network (DQN)** agent to solve the **CarRacing-v3** environment from [Gymnasium](https://gymnasium.farama.org/).  
The agent learns to drive around procedurally generated tracks by **perceiving raw pixels** and optimizing long-term rewards.

<p align="center">
  <img src="https://gymnasium.farama.org/_images/car_racing.gif" width="420"/>
</p>

---

## ğŸ“– Background

CarRacing is a classic benchmark for reinforcement learning:

- **Observation**: top-down 96Ã—96 RGB image of the car and track.  
- **Action space**: continuous `[steering, gas, brake]` in `[-1,1] Ã— [0,1] Ã— [0,1]`.  
- **Challenge**: sparse rewards, exploration difficulty, long horizon, partial observability.  

We adapt this into a **discrete control problem** with wrappers, and train a **Dueling Noisy CNN-based DQN** agent.  
The key ideas:

1. **Convolutional encoder** â†’ extracts spatial features from frames.  
2. **Frame stacking** â†’ gives the agent a sense of motion.  
3. **Replay buffer** â†’ stores experience for stable learning.  
4. **Target network** â†’ reduces Q-value estimation oscillations.  
5. **Exploration** â†’ uses both epsilon-greedy and noisy linear layers.  
6. **Reward shaping** â†’ encourages speed, progress, and smooth driving while penalizing off-track behavior.  

---

## ğŸ“‚ Project Structure
```
.
â”œâ”€â”€ train.py           # Training loop with logging & checkpoints
â”œâ”€â”€ evaluate.py        # Run trained agent with rendering
â”œâ”€â”€ wrappers.py        # Preprocessing, frame stack, reward shaping, action discretization
â”œâ”€â”€ dqn_agent.py       # DQN agent class (policy, target net, optimizer)
â”œâ”€â”€ models.py          # CNN + Dueling + NoisyLinear architecture
â”œâ”€â”€ buffer.py          # Experience replay buffer
â”œâ”€â”€ utils.py           # Helpers (epsilon schedule, seeding, meters)
â”œâ”€â”€ requirements.txt   # Dependencies
â””â”€â”€ runs/              # Logs, checkpoints, and videos (auto-generated)
```

---

## âš™ï¸ Installation

```bash
# clone project
git clone https://github.com/akhilraj96/Car-Racing-DQN-Agent.git
cd carracing-dqn

# create virtual environment
conda create -p env python=3.10 -y
conda activate env/

# install dependencies
python -m pip install --upgrade pip setuptools wheel

pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128

pip install -r requirements.txt
pip install gymnasium[box2d] swig
```

Dependencies include:
- `gymnasium[classic-control, box2d]`
- `numpy`, `opencv-python`
- `torch`, `tensorboard`
- `imageio[ffmpeg, pyav]`
- `tqdm`

---

## ğŸš€ Training

Start training an agent:

- No video (fastest):
```bash
python train.py --video-mode none --run-name dqn_run1
```

- Watch live (slower, renders every step):
```bash
python train.py --video-mode live --run-name dqn_run1
```

- Record one episode every 50 episodes:
```bash
python train.py --video-mode record --record-every 50 --run-name dqn_run1
```

### Key arguments
- `--run-name NAME` â†’ name for logs/checkpoints (default: `dqn_run`)  
- `--total-steps N` â†’ number of environment steps (default: 1M)  
- `--video-mode [none|live|record]` â†’ rendering mode  
- `--save-every N` â†’ checkpoint frequency (default: 50k)  
- `--record-every N` â†’ save evaluation videos every N episodes  

Checkpoints and logs will be saved in:
```
runs/dqn_run1/checkpoints/
runs/dqn_run1/videos/
```

---

## ğŸ® Evaluation

Run a trained model:

```bash
python evaluate.py --checkpoint runs/dqn_run1/checkpoints/agent0/latest.pt --episodes 10
```

Options:
- `--episodes N` â†’ how many test runs  
- `--sleep T` â†’ delay between rendered frames  

---

## ğŸ“Š Monitoring

Training logs are tracked with **TensorBoard**:

```bash
tensorboard --logdir runs/
```

Youâ€™ll see:
- Episode returns  
- Per-step rewards  
- TD-loss curves  
- Exploration epsilon decay  

---

## ğŸ§© Environment Wrappers

To make learning tractable, we apply wrappers (`wrappers.py`):

1. **PreprocessFrame** â†’ grayscale + resize (84Ã—84)  
2. **FrameStack** â†’ stack last 4 frames  
3. **DiscreteActionWrapper** â†’ map continuous controls â†’ 9 discrete actions:
   - no-op, gas, brake  
   - left, right  
   - left+gas, right+gas  
   - left+brake, right+brake  
4. **StickyActions** â†’ repeat last action with prob 0.25 (adds stochasticity)  
5. **Reward shaping**:  
   - Base reward from CarRacing  
   - Speed bonus (encourages fast driving)  
   - Off-track penalty (discourages leaving road)  
   - Control penalties (penalize idling, braking, excessive steering)  
   - Progress reward (tile completion)  
   - Smoothness bonus (discourages jerky driving)  
   - Small time penalty  

---

## ğŸ§  Model Architecture

`DuelingNoisyCNN` (in `models.py`):

- **Convolutional encoder** (NatureCNN style):
  - Conv1: 32 Ã— 8Ã—8 stride 4  
  - Conv2: 64 Ã— 4Ã—4 stride 2  
  - Conv3: 64 Ã— 3Ã—3 stride 1  
- **Dueling heads**:
  - Value stream â†’ scalar V(s)  
  - Advantage stream â†’ A(s,a)  
  - Q(s,a) = V(s) + (A(s,a) âˆ’ mean(A))  
- **NoisyLinear layers** â†’ parametric exploration (Fortunato et al., 2018)  

This combination improves stability and exploration.

---

## ğŸ”¬ Algorithm

The training loop (`train.py`) implements:
- **Epsilon decay** â†’ from 1.0 â†’ 0.05 over 200k steps  
- **Replay buffer** â†’ 200k transitions  
- **Mini-batch training** â†’ batch size 64  
- **Discount factor** â†’ Î³ = 0.995  
- **Optimizer** â†’ Adam, LR = 1e-4  
- **Target network update** â†’ every 2000 steps  
- **TD loss** â†’ smooth L1 (Huber)  

---

## ğŸ“Š Results (Example)

> Your results will vary depending on seeds and hyperparams.

- With reward shaping, the agent learns to **stay on track** within ~200k steps.  
- By ~500k steps, it learns to **accelerate, steer smoothly, and complete laps**.  
- Average returns typically reach **600â€“800+** (solving threshold is 900).  

---

## ğŸ”® Future Work
- âœ… Double DQN  
- âœ… Prioritized Experience Replay (PER)  
- âœ… Dueling DQN (already included)  
- âœ… Noisy Networks for exploration (already included)  
- â¬œ Rainbow DQN integration  
- â¬œ Compare against PPO/SAC  

---

## ğŸ“œ License
MIT License. Free to use, modify, and share.
